# -*- coding: utf-8 -*-
"""Aula 2 - Detecção de Spams.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1w-PRFKUv3-Esu73JYMJMxP-G3DElvhPS

# Detecção de Spams

1. Ler dados
2. Processar textos
3. Representação Textual
4. Classificação
5. Métricas

## 1. Ler dados
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

base = pd.read_csv('drive/My Drive/spam.csv', encoding='latin-1')
base.dropna(how="any", inplace=True, axis=1)
base.columns = ['label', 'message']

base.head()

base['label'].value_counts()

base['label_num'] = base.label.map({'ham':0, 'spam':1})

base.head()

"""## 2. Processamento do texto

1. Deixar tudo em minúsculo
2. Remover símbolos fora do alfabeto
3. Tokenizer
4. Remover Palavras Vazias
5. Normalizar 
6. Retornar para texto

### 1. Deixando tudo em minúsculo
"""

mensagens = base['message']

mensagens[0:5]

from tqdm.notebook import tqdm
tqdm.pandas()

mensagens = mensagens.progress_apply(lambda m: m.lower())

mensagens[0:5]

"""### 2. Removendo símbolos"""

def strip_html(text):
	soup = BeautifulSoup(text, "html.parser")
	return soup.get_text()

#Removing the square brackets
def remove_between_square_brackets(text):
	return re.sub('\[[^]]*\]', '', text)

def remove_special_characters(text, remove_digits=True):
	pattern=r'[^a-zA-z0-9\s]'
	text=re.sub(pattern,'',text)
	return text

from bs4 import BeautifulSoup
import re,string,unicodedata

#Removing the noisy text
def denoise_text(text):
	text = strip_html(text)
	text = remove_between_square_brackets(text)
	text = remove_special_characters(text)
	return text

mensagens = mensagens.progress_apply(denoise_text)

mensagens[0:5]

"""### 3. Tokenizer"""

mensagens = mensagens.progress_apply(lambda m: m.split(' '))

mensagens[0:5]

"""### 4. Removendo palavras vazias (stop words)"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

def remove_stopwords(tokens, language='english'):
	stopword = stopwords.words(language)
	text = [word for word in tokens if word not in stopword]
	return text

mensagens = mensagens.progress_apply(remove_stopwords)

mensagens[0:5]

"""###5. Normalizar

#### Com Lemmatizing
"""

import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

def lemmatizing(tokens):
	wn = WordNetLemmatizer()
	text = [wn.lemmatize(word) for word in tokens]
	return text

mensagens_lem = mensagens.progress_apply(lemmatizing)

mensagens_lem[0:5]

"""#### Com Stemming"""

from nltk.stem.porter import PorterStemmer

def stemming(tokens):
	ps = PorterStemmer()
	text = [ps.stem(word) for word in tokens]
	return text

mensagens_stem = mensagens.progress_apply(stemming)

mensagens_stem[0:5]

"""### Retornar para texto"""

mensagens = mensagens.progress_apply(lambda t: ' '.join(t))
mensagens_lem = mensagens_lem.progress_apply(lambda t: ' '.join(t))
mensagens_stem = mensagens_stem.progress_apply(lambda t: ' '.join(t))

mensagens[0: 5]

mensagens_lem[0:5]

mensagens_stem[0:5]

base['message_sem_norm'] = mensagens
base['message_lem'] = mensagens_lem
base['message_stem'] = mensagens_stem

base.head()

"""## Representação do texto & Classificação"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

x_train, x_test = train_test_split(mensagens_lem, test_size=0.2, random_state=42)
vectorizer = TfidfVectorizer(ngram_range=(1, 1), analyzer='char', max_features=100)
vectorizer.fit(x_train)

vectorizer.vocabulary_

vectors = vectorizer.transform(x_train).todense()

vectors[0]

from sklearn.model_selection import KFold
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import LabelBinarizer
from sklearn.svm import SVC, LinearSVC
from sklearn import metrics

def classificacao(corpus, y):
	split = 5
	kf = KFold(n_splits=split, shuffle=True, random_state=0)
	y_t = []
	y_p = []

	for train_index, test_index in tqdm(kf.split(corpus), total=split):
		# print("TRAIN:", train_index, "TEST:", test_index)
		
		x_train, x_test = corpus[train_index], corpus[test_index]
		vectorizer = TfidfVectorizer(ngram_range=(1, 1))
		# vectorizer = CountVectorizer()
		
		x_train = vectorizer.fit_transform(x_train)
		x_test = vectorizer.transform(x_test)
		
		# print(x_train.shape, x_test.shape)

		y_train, y_test = y[train_index], y[test_index]

		clf = SVC(kernel='linear')
		# clf = LinearSVC()
		clf.fit(x_train, y_train.ravel())
		y_pred = clf.predict(x_test)

		y_t.extend(y_test)
		y_p.extend(y_pred)

	return y_t, y_p

y_t, y_p = classificacao(base['message_lem'], base['label_num'])

import numpy as np
matriz_confusao = metrics.confusion_matrix(y_t, y_p, labels=[1, 0])

print('Matrix de Confusão')
print(matriz_confusao)

import seaborn as sn
import matplotlib.pyplot as plt
pd.set_option('display.float_format', lambda x: '%.3f' % x)

df_cm = pd.DataFrame(matriz_confusao, range(2), range(2))
sn.set(font_scale=1.4) # for label size
labels = ['Spam', 'Ham']
sn.heatmap(df_cm, annot=True, annot_kws={"size": 16, }, fmt='g', xticklabels=labels, yticklabels=labels) # font size
plt.show()

print(f'Acuracia: {metrics.accuracy_score(y_t, y_p)}')
print(f'Precisao: {metrics.precision_score(y_t, y_p)}')
print(f'Revocação: {metrics.recall_score(y_t, y_p)}')
print(f'F1-Score: {metrics.f1_score(y_t, y_p)}')